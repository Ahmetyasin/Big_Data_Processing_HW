{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EJ33bWWHFS00"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "_CTSddhVJL7F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vIwyjdjwJMKP",
        "outputId": "f13583a0-2748-4e12-df43-26a8fc98b513"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.2.4-bin-hadoop3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import FloatType\n",
        "import numpy as np\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"UserSimilarity\").getOrCreate()\n",
        "\n",
        "# Load the ratings data\n",
        "ratings_df = spark.read.csv(\"/content/ratings.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Select only necessary columns and cast them to appropriate types\n",
        "ratings_df = ratings_df.select(\n",
        "    col(\"userId\").cast(\"int\"),\n",
        "    col(\"movieId\").cast(\"int\"),\n",
        "    col(\"rating\").cast(\"float\")\n",
        ")\n",
        "\n",
        "# Define ALS model\n",
        "als = ALS(\n",
        "    maxIter=10,\n",
        "    regParam=0.01,\n",
        "    userCol=\"userId\",\n",
        "    itemCol=\"movieId\",\n",
        "    ratingCol=\"rating\",\n",
        "    coldStartStrategy=\"drop\",\n",
        "    nonnegative=True\n",
        ")\n",
        "\n",
        "# Fit ALS model to the data\n",
        "model = als.fit(ratings_df)\n",
        "\n",
        "# Extract feature vectors for users\n",
        "user_features = model.userFactors\n",
        "\n",
        "# Defining cosine_similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n",
        "\n",
        "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
        "\n",
        "\n",
        "# Compute pairwise similarities\n",
        "users_cartesian = user_features.alias(\"uf1\").crossJoin(user_features.alias(\"uf2\"))\n",
        "similarity_df = users_cartesian.withColumn(\"similarity\", cosine_similarity_udf(col(\"uf1.features\"), col(\"uf2.features\")))\n",
        "\n",
        "# Add user IDs back to the similarity dataframe\n",
        "similarity_df = similarity_df.select(\n",
        "    col(\"uf1.id\").alias(\"userId1\"),\n",
        "    col(\"uf2.id\").alias(\"userId2\"),\n",
        "    col(\"similarity\")\n",
        ")\n",
        "\n",
        "# Extract top 10 similar users for each user\n",
        "windowSpec = Window.partitionBy(\"userId1\").orderBy(col(\"similarity\").desc())\n",
        "top_similar_users = similarity_df.withColumn(\"rank\", rank().over(windowSpec)) \\\n",
        "                                 .filter(col(\"rank\") <= 11) \\\n",
        "                                 .filter(col(\"userId1\") != col(\"userId2\"))\n",
        "\n",
        "# Show or save results\n",
        "top_similar_users.select(\"userId1\", \"userId2\", \"similarity\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXS4h34uJNIx",
        "outputId": "5a8e397d-da1d-4d3c-a0dd-50fa726e9dd5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----------+\n",
            "|userId1|userId2|similarity|\n",
            "+-------+-------+----------+\n",
            "|      1|    119| 0.9447821|\n",
            "|      1|     54| 0.9331167|\n",
            "|      1|    137| 0.9270684|\n",
            "|      1|    273| 0.9262865|\n",
            "|      1|    349| 0.9169113|\n",
            "|      1|    169|0.91677123|\n",
            "|      1|    453|0.91462857|\n",
            "|      1|    401| 0.9142054|\n",
            "|      1|    515| 0.9130469|\n",
            "|      1|    484| 0.9116573|\n",
            "|      3|    567| 0.8955833|\n",
            "|      3|    301| 0.8747417|\n",
            "|      3|    442| 0.8595087|\n",
            "|      3|    245| 0.8054378|\n",
            "|      3|    310|0.77963835|\n",
            "|      3|    394|0.77755773|\n",
            "|      3|    106|0.73008585|\n",
            "|      3|    299| 0.7245862|\n",
            "|      3|    375| 0.7101733|\n",
            "|      3|     16|0.70192784|\n",
            "+-------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, I kept getting memory errors for cosine similarity and other similarity metrics, I had to use another method. For that purpose, I found ALS model to be useful.\n",
        "\n",
        "ALS is preferred for large, sparse datasets like user-movie ratings because it efficiently handles sparse data and scales well with large datasets. It extracts latent features that represent underlying user preferences and item characteristics, providing deeper insights than direct rating comparisons.\n"
      ],
      "metadata": {
        "id": "zIlsDKLqPKHe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8wyH35AxPDTd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}